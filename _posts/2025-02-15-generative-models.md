---
layout: post
title: Generative Models
date: 2025-02-15 21:10 +0800
pin: true
math: true
mermaid: true
---


## Introduction of Generative Models [[1]](#references)

When discussing **generative models**, it's essential to understand how machine learning approaches tasks. Consider a scenario where we aim to distinguish between elephants and dogs, there are primarily two modeling approaches: discriminative and generative:

1. **Discriminative Modeling:** This approach involves building a model that directly predicts classification labels or identifies the decision boundary between elephants and dogs.
2. **Generative Modeling:** This approach entails constructing separate models for elephants and dogs, capturing their respective characteristics. A new animal is then compared against each model to determine which it resembles more closely.

In discriminative modeling, the focus is on learning the conditional probability of labels given the input data, denoted as $$ p(y\mid{x}). $$ Techniques like logistic regression exemplify this by modeling the probability of a label based on input features. Alternatively, methods such as the perceptron algorithm aim to find a decision boundary that maps new observations to specific labels $$\{0,1\}$$, such as 0 for dogs and 1 for elephants.

Conversely, generative modeling focuses on understanding how the data is generated by learning the joint probability distribution 
$$p(x,y)$$ or the likelihood $$p(x\mid{y})$$ along with the prior probability $$p(y)$$. This approach models the distribution of the input data for each class, enabling the generation of new data points and facilitating classification by applying Bayes' theorem to compute the posterior probability $$ p(y\mid{x})=p(x\mid{y})p(y)/p(x) $$.







## References

[1] Ng, Andrew. "CS229: Machine Learning Course Notes." Stanford University, 2018.


